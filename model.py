# -*- coding: utf-8 -*-
"""sentiment-based-product-recommendation-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XFM_EjRuSd7ab_JgumETivT2u1WG84x5
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
import datetime


import nltk
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import *
from sklearn import *

import joblib

from imblearn.over_sampling import SMOTE

import pickle

import warnings
warnings.filterwarnings('ignore')



## Reading the csv file
reviews = pd.read_csv("data/sample30.csv")

## Displaying the head
reviews.head()

"""# Data Cleansing and Pre-processing"""

## Checking the shape of the data frame
reviews.shape

## Checking the info of the data set
reviews.info()

## Checking for the null value counts
reviews.isnull().sum()

## Checking the Null value count percentages
round(100*reviews.isnull().sum()/reviews.shape[0],2)

## Dropping the columns reviews_userCity and reviews_userProvince as they have more than 90% null values and seems to be no importance for the analysis
reviews = reviews.drop(['reviews_userCity','reviews_userProvince'],axis=1)
reviews.shape

## checking again the null value percentages
round(100*reviews.isnull().sum()/reviews.shape[0],2)

## Displaying the head
reviews.head()

## Checking the user_sentiment which has null value
reviews[reviews['user_sentiment'].isnull()]

"""## Imputing default values for the Null Values"""

## There are Null values in manufacturer column and we can impute the Brand values as Brand and Manufacturer seems to have similarity
reviews['manufacturer'] = np.where(reviews['manufacturer'].isnull(),reviews['brand'],reviews['manufacturer'])

## There is one record with user_sengtiment as Null and imputed manually with Positive based on the reviews_rating column.
reviews['user_sentiment'] = np.where(reviews['user_sentiment'].isnull(),'Positive',reviews['user_sentiment'])

## reviews_didPurchase and reviews_doRecommend has Null values and I will impute them with "N/A" 
reviews['reviews_didPurchase'] = reviews['reviews_didPurchase'].fillna('N/A')
reviews['reviews_doRecommend'] = reviews['reviews_doRecommend'].fillna('N/A')

## We could see there is null values for Reviews_date and can be imputed with default date.
reviews['reviews_date'] = reviews['reviews_date'].fillna('1969-12-31T00:00:00.000Z')

## There are some records with reviews_username as Null and i will impute with "Anonymus" user.
reviews['reviews_username'] = reviews['reviews_username'].fillna('Anonymus')

## Imputing Niull values in reviews_title with reviews_text as title also has similar text
reviews['reviews_title'] = np.where(reviews['reviews_title'].isnull(),reviews['reviews_text'],reviews['reviews_title'])

def is_date_matching(date_str):
    try:
        return datetime.datetime.strptime(date_str, '%Y-%m-%d')
    except ValueError:
        return datetime.datetime.strptime('1969-12-31', '%Y-%m-%d')

## Extracting date part alone as timestamp part is not required for our analysis
reviews['reviews_date'] = reviews['reviews_date'].apply(lambda x: x.split('T')[0])
## Cleaning date column with proper default date and creating reviews_year column for analysis
reviews['reviews_date'] = reviews['reviews_date'].apply(lambda x: is_date_matching(x))
reviews['reviews_year'] = reviews['reviews_date'].apply(lambda x: x.year)

## Checking for the transformed columns
reviews.head()

"""## Tranforming Target column"""

## Tranforming the user_sentiment from Categorical to Numeric as this is our prediction variable
reviews['user_sentiment'] = reviews['user_sentiment'].map({'Positive': 1, 'Negative': 0}).astype('int')

## Pre-processing the reviews_text
## Keeping only alphabets which are necessary for classification. Also convert the text to lower case
reviews['reviews_text'] = reviews['reviews_text'].apply(lambda x: re.sub('[^a-zA-Z]+', ' ', x.lower()).strip())

## Downloading required nltk libraries
nltk.download('wordnet')
nltk.download('punkt')

"""### Lemmatization"""

## Tokenize and Applying Lemmatization to the review_text
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

wordnet_lemmatizer = WordNetLemmatizer()
reviews['reviews_text'] = reviews['reviews_text'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(token, pos='n') for token in word_tokenize(x)]))

## Lets check again info of the data
reviews.info()

"""# Model Building"""

## Assigning X,y,seed values for the model building 
seed = 42
X = reviews['reviews_text']
y = reviews['user_sentiment']

"""# Feature Extraction

"""

## Getting the stowords from nltk for Text analysis
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

"""## TF-IDF"""

## Getting the features
tfidf_vec = TfidfVectorizer(
    max_features=None, 
    lowercase=True, 
    analyzer='word', 
    stop_words= stop_words, 
    ngram_range=(1, 1))

"""# Logistic Regression"""

# Checking class imbalnce is present or not
y.value_counts()

"""## Above clearly shows there is imbalance in the sentiments. I will apply SMOTE technique to overcome that after splitting into Train and Test daat sets."""

## Splitting the data to train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)

## Applying TF-IDF to extract train, test feature set
tfidf_vec.fit(X_train)
X_train = tfidf_vec.transform(X_train)
X_test = tfidf_vec.transform(X_test)

## Applying SMOTE for imbalance data
sm = SMOTE()
X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)
X_test_smote, y_test_smote = sm.fit_sample(X_test, y_test)

## Logistic Regression
model = LogisticRegression(penalty="l2", random_state=seed, max_iter=500)

## Model fit
model.fit(X_train_smote, y_train_smote)

## Model Prediction
y_pred = model.predict(X_test_smote)

## Accuracy
train_accuracy = model.score(X_train_smote, y_train_smote)
test_accuracy = model.score(X_test_smote, y_test_smote)

## Classification report
model_performance = classification_report(y_test_smote, y_pred)

y_pred_proba = model.predict_proba(X_test_smote)
roc_auc = roc_auc_score(y_test_smote, y_pred_proba[:,1])


print('')
print(model_performance)
print('')

print ('Training Accuracy: ', round(train_accuracy,2))
print ('Testing Accuracy: ', round(test_accuracy,2))
print('ROC_AUC score: ', round(roc_auc,2))

"""## Save the ML Model"""

# Save the model as a pickle in a file 
joblib.dump(model, 'models/lr_model.pkl')
# Save the TF-IDF model as a pickle in a file 
joblib.dump(tfidf_vec, 'models/tfidf_model.pkl')

"""## Load the ML Model"""

# Load the model from the file 
lr_pickle_model = joblib.load('models/lr_model.pkl')
# Load the model from the file 
tfidf_pickle_model = joblib.load('models/tfidf_model.pkl')

# Use the loaded model to make predictions
response_lr_model = lr_pickle_model.predict(X_test)

"""## Evaluate the Loaded Model"""

# predict sentiment on test data
y_pred_load = lr_pickle_model.predict(X_test)

# create onfusion matrix
cm_load = confusion_matrix(y_test, y_pred_load)
print(cm_load)

#Checking the accuracy
print("Accuracy: \t",round(lr_pickle_model.score(X_test, y_test),2))

# check area under curve
y_pred_prob_load = lr_pickle_model.predict_proba(X_test)[:, 1]
print("ROC:    \t", round(roc_auc_score(y_test, y_pred_prob_load),2))

"""# Product Recommendation

### Now lets get the top 15 products from the Recommendation system.
### We are going to execute Item-based Recommendation system

### Dividing the dataset into train and test
"""

## Taking the required columns - username,brand(product) and ratings from the data set
df_recommend = pd.DataFrame(data = reviews[['reviews_username','id','reviews_rating']])
## Checking the data set
df_recommend.head()

## If there are any duplicate ratings present for user and product combination then lets apply mean and take the average rating
df_recommend = df_recommend.groupby(['reviews_username','id'],as_index=False).agg({'reviews_rating':pd.Series.mode,'reviews_rating':np.mean})

## Changing column names for better understanding and representation
df_recommend['username'] = df_recommend['reviews_username']
df_recommend['product'] = df_recommend['id']
df_recommend['rating'] = round(df_recommend['reviews_rating'],1)
## Dropping the old columns and checking the data
df_recommend = df_recommend.drop(['reviews_username','id','reviews_rating'],axis=1)
df_recommend.head()

## Splitting the data into train and test sets
train, test = train_test_split(df_recommend, test_size=0.2, random_state=seed)
## check the shape of the data sets
print(train.shape)
print(test.shape)

## Checking info of the data frame
df_recommend.info()

# Apply pivot to see the user ratings for each product and give '0' wherever rating is not provided by the user
df_product_features = train.pivot(
    index='username',
    columns='product',
    values='rating'
).fillna(0)

## Checking the data set after pivoting the data frame
df_product_features.head()

"""# Copy train and test dataset

### These dataset will be used for prediction and evaluation.


*   Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the products rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction.
*   Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train


"""

## Taking the copy of train and test data sets
dummy_train = train.copy()
dummy_test = test.copy()

## Changing the rating value to '0' if the user provided rating
dummy_train['rating'] = dummy_train['rating'].apply(lambda x: 0 if x>=1 else 1)
## Changing the rating value to '1' if the user provided rating
dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x>=1 else 0)

# The products not rated by user is marked as 1 for prediction. 
dummy_train = dummy_train.pivot(
    index='username',
    columns='product',
    values='rating'
).fillna(1)

# The products not rated by user is marked as 0 for evaluation. 
dummy_test = dummy_test.pivot(
    index='username',
    columns='product',
    values='rating'
).fillna(0)

## Lets check the train data set
dummy_train.head()

## Lets check the test data set
dummy_test.head()

"""# Item Based Similarity"""

## Taking the transpose of the reviews matrix to normalize the rating around the mean for different product ID.
## In the user based similarity, we had taken mean for each user intead of each product. 
product_features = train.pivot(
    index='username',
    columns='product',
    values='rating'
).T

product_features.head()

## Normalising the product rating for each product
mean = np.nanmean(product_features, axis=1)
df_subtracted = (product_features.T-mean).T

df_subtracted.head()

## Finding the cosine similarity. Note that since the data is normalised, both the cosine metric and correlation metric will give the same value. 
from sklearn.metrics.pairwise import pairwise_distances

# User Similarity Matrix
item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
item_correlation[np.isnan(item_correlation)] = 0
print(item_correlation)

## Filtering the correlation only for which the value is greater than 0. (Positively correlated)
item_correlation[item_correlation<0]=0
item_correlation

"""# Prediction"""

item_predicted_ratings = np.dot((product_features.fillna(0).T),item_correlation)
item_predicted_ratings

item_predicted_ratings.shape

dummy_train.shape

"""### Filtering the rating only for the products not rated by the user for recommendation"""

item_final_rating = np.multiply(item_predicted_ratings,dummy_train)
item_final_rating.head()

"""## Top 15 prediction for the user -1"""

item_final_rating.iloc[1].sort_values(ascending=False)[0:15]

"""# Evaluation

### Here lets evaluate for the product which is already rated by the user insead of predicting it for the product not rated by the user.

# Using Item similarity
"""

test_product_features = test.pivot(
    index='username',
    columns='product',
    values='rating'
).T

mean = np.nanmean(test_product_features, axis=1)
test_df_subtracted = (test_product_features.T-mean).T

test_item_correlation = 1 - pairwise_distances(test_df_subtracted.fillna(0), metric='cosine')
test_item_correlation[np.isnan(test_item_correlation)] = 0
test_item_correlation[test_item_correlation<0]=0

test_item_correlation.shape

test_product_features.shape

test_item_predicted_ratings = (np.dot(test_item_correlation, test_product_features.fillna(0))).T
test_item_final_rating = np.multiply(test_item_predicted_ratings,dummy_test)
test_item_final_rating.head()

test_ = test.pivot(
    index='username',
    columns='product',
    values='rating'
)

from sklearn.preprocessing import MinMaxScaler
from numpy import *

X  = test_item_final_rating.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))


test_ = test.pivot(
    index='username',
    columns='product',
    values='rating'
)

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

"""# Finding RMSE"""

rmse = (sum(sum((test_ - y )**2))/total_non_nan)**0.5
print(rmse)

## Checking the shape of the final recommendations
item_final_rating.shape

final_pred_df = pd.DataFrame()
for i in np.arange(0,20254):  
    ## Getting the top 15 products for each user
    df = item_final_rating.iloc[i].sort_values(ascending=False)[:15]

    ## Removing the products which dont have any weightage calculated
    product_id = np.array(df.index[df.values != 0])
    user_name = item_final_rating.iloc[i].name

    ##Creating and storing in data frame at user_name and product
    sorted_user_predictions = pd.DataFrame({'product_id':product_id,'user_name':user_name})
    sorted_user_predictions.assign(product_id=sorted_user_predictions.product_id.str.split(",")).explode("product_id")

    sorted_user_predictions.set_index('product_id')
    final_pred_df = final_pred_df.append(sorted_user_predictions)

"""## Saving the Recommendations in pickle file"""

# open a file, where you ant to store the data
file = open('/models/product_recommend.pkl', 'wb')

# dump information to that file
pickle.dump(final_pred_df, file)

# close the file
file.close()

"""## Saving the user reviews as pickle file"""

prod_review_df = reviews[['id','name','reviews_text']]

# open a file, where you ant to store the data
file = open('models/reviews_text.pkl', 'wb')

# dump information to that file
pickle.dump(prod_review_df, file)

# close the file
file.close()

"""## Test the pickle file working or not for sample users"""

# open a file, where you stored the pickled data
file = open('models/product_recommend.pkl', 'rb')

# dump information to that file
product_data = pickle.load(file)

# close the file
file.close()

# open a file, where you stored the pickled data
file = open('models/reviews_text.pkl', 'rb')

# dump information to that file
reviews_data = pickle.load(file)

# close the file
file.close()

user_recom = product_data[product_data['user_name'] == '00dog3']
user_recom

## Getting the reviews for the products recomended for the user
df = reviews_data.join(user_recom.set_index('product_id'),on=['id'],how="inner")

## Checking the shape
df.shape

## Getting the reviews for predicting the sentiments
X_test_user = df['reviews_text']
X_test_user = tfidf_pickle_model.transform(X_test_user)
X_test_user.shape

## Model prediction
y_pred_user = lr_pickle_model.predict(X_test_user)

## Predictions
y_pred_user

## printing the shapes of input and ouput data
print(df.shape)
print(y_pred_user.shape)

## Merging input and output to get the final results
user_sentiment = pd.DataFrame(data=y_pred_user, columns=['user_sentiment'], 
                            index=df.index.copy())
fnl_result = pd.merge(df, user_sentiment, how ='left',left_index = True, right_index = True)

## Checking the shape
fnl_result.shape

## Getting the products based on top 5 most positively reviewd using mean
new_df = fnl_result.groupby('name')['user_sentiment'].mean()
new_df.sort_values(ascending=False)[:5].index

